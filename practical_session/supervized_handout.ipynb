{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apprentissage supervisé\n",
    "\n",
    "Le but de ce TP est d'implémenter et tester les algorithmes vues en classes.\n",
    "\n",
    "## Utilitaire\n",
    "\n",
    "Ci-dessous des fonctions à utiliser pour le TP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def plot_points(points, ax, color):\n",
    "    \"\"\"\n",
    "        Plots points.\n",
    "        :param points: the points to plot\n",
    "        :type points: iterable\n",
    "        :param ax: the axis to plot in\n",
    "        :type ax: matplotlib.axis\n",
    "        :param color: the plotted points color\n",
    "        :type color: str\n",
    "    \"\"\"\n",
    "    ax.scatter(points[:, 0], points[:, 1], c=color)\n",
    "\n",
    "    \n",
    "def plot_dataset(X, Y, ax, **parameters):\n",
    "    \"\"\"\n",
    "        Plots the dataset.\n",
    "        :param X: the points to plot\n",
    "        :type X: np.array\n",
    "        :param Y: the points classes\n",
    "        :type Y: np.array\n",
    "        :param ax: the axis to plot in\n",
    "        :type ax: matplotlib.axis\n",
    "        :param colors: the plotted points colors depending on the class\n",
    "        :type colors: iterable\n",
    "    \"\"\"\n",
    "    for x, color in zip([X[Y==y] for y in set(Y)], parameters['colors']):\n",
    "        plot_points(x, ax, color)\n",
    "\n",
    "        \n",
    "def mesh_from(X, number=100):\n",
    "    \"\"\"\n",
    "        Constructs mesh points from some 2D instances\n",
    "        :param X: the 2D instances\n",
    "        :type X: np.array\n",
    "        :param number: the quatization frequency\n",
    "        :type number: int\n",
    "        :return: the mesh points\n",
    "        :rtype: tuple\n",
    "    \"\"\"\n",
    "    return np.meshgrid(\n",
    "        np.linspace(X[:, 0].min() - 1, X[:, 0].max() + 1, number),\n",
    "        np.linspace(X[:, 1].min() - 1, X[:, 1].max() + 1, number),\n",
    "    )\n",
    "\n",
    "\n",
    "def plot_separator(xx, yy, ax, classifier, **parameters):\n",
    "    \"\"\"\n",
    "        Plots separator.\n",
    "        \n",
    "        :param xx: mesh first coordinates\n",
    "        :type xx: np.array\n",
    "        :param yy: mesh second coordinates\n",
    "        :type yy: np.array\n",
    "        :param ax: subplot to draw in\n",
    "        :type ax: matplotlib.axis\n",
    "        :param classifier: the prediction function\n",
    "        :type classifier: callable\n",
    "    \"\"\"\n",
    "    Z = classifier(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    ax.contourf(xx, yy, Z, **parameters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On génère trois bases de données pour l'apprentissage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "import sklearn.decomposition\n",
    "\n",
    "X_blob, Y_blob = sklearn.datasets.make_blobs(n_samples=178, centers=2, random_state=10, cluster_std=5)\n",
    "X_circles, Y_circles = sklearn.datasets.make_circles(n_samples=178, factor=.3, noise=.1)\n",
    "wine_data = sklearn.datasets.load_wine()\n",
    "X_wine, Y_wine = sklearn.decomposition.PCA(n_components=2).fit_transform(wine_data.data), wine_data.target\n",
    "datasets = [('blobs', X_blob, Y_blob), ('circles', X_circles, Y_circles), ('wine', X_wine, Y_wine)]\n",
    "\n",
    "figure, (ax1, ax2, ax3) = plt.subplots(1, 3)\n",
    "figure.set_figheight(10)\n",
    "figure.set_figwidth(30)\n",
    "\n",
    "plot_dataset(X_blob, Y_blob, ax1, colors=['r', 'b'])\n",
    "ax1.set_title('Simple synthetic dataset')\n",
    "plot_dataset(X_circles, Y_circles, ax2, colors=['r', 'b'])\n",
    "ax2.set_title('Circular synthetic dataset')\n",
    "plot_dataset(X_wine, Y_wine, ax3, colors=['r', 'b', 'g'])\n",
    "ax3.set_title('First 2 dimensions of dataset')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifieur de centroïdes\n",
    "\n",
    "1.\n",
    "Implémenter la fonction `centroid_train` qui entraîne un modèle de classifieur de centroïdes.\n",
    "\n",
    "2.\n",
    "Implémenter le fonction `centroid_test` qui prédit, pour une instance, la classe la plus probable.\n",
    "\n",
    "3.\n",
    "Lancer le code et commenter les résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "# Euclidian Norm\n",
    "euclidian = lambda x, y: np.linalg.norm(np.array(x) - np.array(y))\n",
    "\n",
    "def centroid_train(X, Y):\n",
    "    return [\n",
    "        # For each class `cls` compute the centroid\n",
    "        for cls in set(Y)\n",
    "    ]\n",
    "\n",
    "\n",
    "def centroid_test(X, centroids, metric=euclidian):\n",
    "    return np.array(\n",
    "        [\n",
    "            # For each observation `x` predict the class\n",
    "            for x in X\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "figure, axes = plt.subplots(1, 3)\n",
    "figure.set_figheight(10)\n",
    "figure.set_figwidth(30)\n",
    "\n",
    "for (name, X, Y), ax, colors in zip(datasets, axes, [['r', 'b'], ['r', 'b'], ['r', 'b', 'g']]):\n",
    "    plot_dataset(X, Y, ax, colors=colors)\n",
    "    centroids = centroid_train(X, Y)\n",
    "    xx, yy = mesh_from(X, 150)\n",
    "    plot_separator(\n",
    "        xx,\n",
    "        yy,\n",
    "        ax,\n",
    "        lambda x:  centroid_test(x, centroids),\n",
    "        cmap=plt.cm.viridis,\n",
    "        alpha=0.5\n",
    "    )\n",
    "    ax.set_title('Centroid classifier applied to' + name)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-NN\n",
    "\n",
    "4.\n",
    "Utiliser la classe `KNeighborsClassifier` pour compléter le code suivant. Commenter les résultats.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.neighbors\n",
    "\n",
    "for k in range(3,12): # Different k values are tried\n",
    "    figure, axes = plt.subplots(1, 3)\n",
    "    figure.set_figheight(10)\n",
    "    figure.set_figwidth(30)\n",
    "    for (name, X, Y), ax, colors in zip(datasets, axes, [['r', 'b'], ['r', 'b'], ['r', 'b', 'g']]):\n",
    "        plot_dataset(X, Y, ax, colors=colors)\n",
    "        xx, yy = mesh_from(X, 150)\n",
    "        model = None # The classifier model\n",
    "        plot_separator(\n",
    "            xx,\n",
    "            yy,\n",
    "            ax,\n",
    "            lambda x: model.predict(x),\n",
    "            cmap=plt.cm.viridis,\n",
    "            alpha=0.5\n",
    "        )\n",
    "        ax.set_title('k-NN classifier applied to' + name + 'with k=' + str(k))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arbre de décision\n",
    "\n",
    "5.\n",
    "Utiliser la classe `DecisionTreeClassifier` pour compléter le code suivant. Commenter les résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.tree\n",
    "\n",
    "for criterion in ['gini', 'entropy']:\n",
    "    figure, axes = plt.subplots(1, 3)\n",
    "    figure.set_figheight(10)\n",
    "    figure.set_figwidth(30)\n",
    "    for (name, X, Y), ax, colors in zip(datasets, axes, [['r', 'b'], ['r', 'b'], ['r', 'b', 'g']]):\n",
    "        plot_dataset(X, Y, ax, colors=colors)\n",
    "        xx, yy = mesh_from(X, 150)\n",
    "        model = None # The classifier model\n",
    "        plot_separator(\n",
    "            xx,\n",
    "            yy,\n",
    "            ax,\n",
    "            lambda x: model.predict(x),\n",
    "            cmap=plt.cm.viridis,\n",
    "            alpha=0.5\n",
    "        )\n",
    "        ax.set_title('Decision tree classifier applied to ' + name + ' with '+ criterion + ' criterion ')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.\n",
    "Imposer aux classifieur, comme critère d'arrêt, une profondeur maximale. Commenter les résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.tree\n",
    "\n",
    "for max_depth in range(1, 10):\n",
    "    figure, axes = plt.subplots(1, 3)\n",
    "    figure.set_figheight(10)\n",
    "    figure.set_figwidth(30)\n",
    "    for (name, X, Y), ax, colors in zip(datasets, axes, [['r', 'b'], ['r', 'b'], ['r', 'b', 'g']]):\n",
    "        plot_dataset(X, Y, ax, colors=colors)\n",
    "        xx, yy = mesh_from(X, 150)\n",
    "        model = None # The classifier model\n",
    "        plot_separator(\n",
    "            xx,\n",
    "            yy,\n",
    "            ax,\n",
    "            lambda x: model.predict(x),\n",
    "            cmap=plt.cm.viridis,\n",
    "            alpha=0.5\n",
    "        )\n",
    "        ax.set_title('Decision tree classifier applied to ' + name + ' with max depth '+ str(max_depth) + ' criterion ')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.\n",
    "Changer le critère d'arrêt on fixant différent seuil de pureté de feuilles. Commenter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.tree\n",
    "\n",
    "for impurity in [l/20 for l in range(1, 10)]:\n",
    "    figure, axes = plt.subplots(1, 3)\n",
    "    figure.set_figheight(10)\n",
    "    figure.set_figwidth(30)\n",
    "    for (name, X, Y), ax, colors in zip(datasets, axes, [['r', 'b'], ['r', 'b'], ['r', 'b', 'g']]):\n",
    "        plot_dataset(X, Y, ax, colors=colors)\n",
    "        xx, yy = mesh_from(X, 150)\n",
    "        model = None # The classifier model\n",
    "        plot_separator(\n",
    "            xx,\n",
    "            yy,\n",
    "            ax,\n",
    "            lambda x: model.predict(x),\n",
    "            cmap=plt.cm.viridis,\n",
    "            alpha=0.5\n",
    "        )\n",
    "        ax.set_title('Decision tree classifier applied to ' + name + ' with '+ str(impurity) + ' maximum impurity criterion')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forêt aléatoire\n",
    "\n",
    "8.\n",
    "Compléter les fonctions `select_attributes`, `random_forest_train` et `random_forest_test`, puis lancer le code suivant. Commenter les résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def sampling_indices(X):\n",
    "    # Draw a random number of dimensions\n",
    "    d = random.randint(1, X.shape[1])\n",
    "    # return indices to be sampled\n",
    "    return random.sample(range(X.shape[1]), d)\n",
    "\n",
    "\n",
    "def majority_vote(trees_predictions):\n",
    "    return [\n",
    "        max(\n",
    "            [\n",
    "                (cls, sum(tree_predictions == cls))\n",
    "                for cls in set(tree_predictions)\n",
    "            ],\n",
    "            key=operator.itemgetter(-1)\n",
    "        )[0]\n",
    "        for tree_predictions in trees_predictions\n",
    "    ]\n",
    "\n",
    "\n",
    "def select_attributes(X, trees_sampling_indices):\n",
    "    return [\n",
    "        # For each tree index select the attributes in the sampling indices. \n",
    "        # Must be a couple (tree, attributes)\n",
    "        for tree_idx, sampling_indices in trees_sampling_indices\n",
    "    ]\n",
    "\n",
    "\n",
    "def random_forest_train(X, Y, number_trees=100, max_depth=4):\n",
    "    trees_sampling_indices = [\n",
    "        # For each tree index draw the sampling indices.\n",
    "        # Must be a couple (tree, indices). Use the function `sampling_indices()`\n",
    "        for tree_idx in range(number_trees)\n",
    "    ]\n",
    "    X_as = select_attributes(X, trees_sampling_indices)\n",
    "    trained_trees = {\n",
    "        tree_idx: # fitted tree with max depth `max_depth`\n",
    "        for (tree_idx, X_a) in X_as\n",
    "    }\n",
    "    return (\n",
    "        trees_sampling_indices,\n",
    "        trained_trees\n",
    "    )\n",
    "\n",
    "\n",
    "def random_forest_test(X, trees_sampling_indices, trees):\n",
    "    X_as = select_attributes(X, trees_sampling_indices)\n",
    "    trees_predictions = np.array(\n",
    "        [\n",
    "            # For each tree predict the classe of each observation\n",
    "            for tree_idx, X_a in X_as\n",
    "        ]\n",
    "    ).transpose()\n",
    "    return np.array(\n",
    "        majority_vote(trees_predictions)\n",
    "    )\n",
    "\n",
    "\n",
    "figure, axes = plt.subplots(1, 3)\n",
    "figure.set_figheight(10)\n",
    "figure.set_figwidth(30)\n",
    "\n",
    "for (name, X, Y), ax, colors in zip(datasets, axes, [['r', 'b'], ['r', 'b'], ['r', 'b', 'g']]):\n",
    "    plot_dataset(X, Y, ax, colors=colors)\n",
    "    trees_sampling_indices, trees = random_forest_train(X, Y, number_trees=500, max_depth=1)\n",
    "    xx, yy = mesh_from(X, 150)\n",
    "    plot_separator(\n",
    "        xx,\n",
    "        yy,\n",
    "        ax,\n",
    "        lambda x:  random_forest_test(x, trees_sampling_indices, trees),\n",
    "        cmap=plt.cm.viridis,\n",
    "        alpha=0.5\n",
    "    )\n",
    "    ax.set_title('Random Forest classifier applied to' + name)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
